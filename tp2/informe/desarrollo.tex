\subsection{Reconocimiento óptico de caracteres}

La tecnología de reconocimiento de caracteres, OCR (Optical Character  Recognition)  engloba  a  un  conjunto  de  técnicas basadas en  estadísticas, en las formas de los caracteres, transformadas y en comparaciones, que complementándose entre sí, se  emplean para  distinguir de forma automática entre  los diferentes caracteres alfanuméricos existentes.
En realidad no se reconocen  exactamente los  caracteres de un determinado alfabeto, sino que es posible distinguir entre cualquier conjunto de formas o símbolos. Sin embargo, se debe tener en cuenta que la precisión que se obtiene en la práctica al intentar distinguir entre un conjunto de símbolos no es total. Por lo tanto, es fácil  deducir  que  cuanto  más  numeroso  es  el conjunto de símbolos entre los que se debe decidir, mayor es la probabilidad de que se produzca un fallo de clasificación.
En todo sistema de reconocimiento óptico de caracteres se distinguen al menos estas 4 etapas:
\begin{itemize}
    \item Preproceso
    \item Segmentación
    \item Extracción de características
    \item Reconocimiento
\end{itemize}


\subsubsection{Preproceso}

En esta fase de preprocesamiento (o adecuación de la imagen) el objetivo que se persigue es eliminar de la imagen de cualquier
tipo de ruido o imperfección que no pertenezca al carácter, así como normalizar el tamaño del mismo. La normalización de la imagen también puede implicar un binarizado de la misma. Para la eliminación del ruido que puede aparecer en una imagen digital se utilizan diversos algoritmos:
\begin{itemize}
    \item Etiquetado:  para  la  división  de  la  imagen  en  regiones  de componentes conectadas.
    \item Erosión / expansión: para la eliminación de peque\~nos grupos de píxeles.
    \item Umbralizado de histograma:  para  eliminar/seleccionar los objetos más brillantes o más oscuros de la imagen.
\end{itemize}


\subsubsection{Segmentación}

Una vez preprocesada la imagen se deberá segmentar en las diferentes componentes conexas (parte de la imagen donde todos los píxeles son adyacentes entre sí) que la componen. La segmentación de la imagen constituye una de las mayores dificultades del reconocimiento, y se hace  necesaria para poder reconocer cada uno de los caracteres de la imagen binaria.
La fragmentación o segmentación es la operación que permite la descomposición  de  un  texto  en  diferentes  entidades lógicas. Estas entidades deben ser lo suficientemente invariables, para ser independientes del escritor, y lo suficientemente significativas para su reconocimiento.


\subsubsection{Extracción de características}

Una  vez  realizada  la  segmentación,  se  tiene  una  imagen normalizada en la que se encuentra la información susceptible de ser “reconocida”. La información así representada, una matriz bidimensional de valores binarios.

La extracción de las características es una de las fases  más difíciles, dado que es muy difícil elegir un conjunto de características óptimo.
Para que una característica se pueda considerar buena debe tener:
\begin{itemize}
    \item Discriminación: Deben  ser características que diferencien suficientemente una clase de otra
    \item Deben tener igual valor para mismas clases
    \item Independencia: Las características deben estar incorreladas unas de otras.
    \item Pequeño espacio para características: El número de características debe ser pequeño para la rapidez y facilidad de clasificación.
\end{itemize}

En este trabajo desarrollaremos y evaluaremos dos técnicas.


{\ttfamily PCA (Principal Component Analysis)}

El objetivo de esta técnica es definir una transformación lineal desde el espacio de representación original a un nuevo espacio en el que las distintas clases de las muestras quedan mejor separadas.
Esta transformación permite reducir la dimensión del nuevo espacio sin perjudicar sensiblemente la capacidad discriminativa de la nueva representación.

{\ttfamily PSL-DA (Partial Least Squares Discriminant Analysis)}

Es un método estadístico que tiene relación con la regresión de componentes principales, se encuentra una regresión lineal mediante la proyección de las variables de predicción y las variables observables a un nuevo espacio. Debido a que tanto los datos de X e Y se proyectan a nuevos espacios, los familia de los modelos PLS se conoce como factor de modelos bilineales. Los cuadrados mínimos parciales Análisis discriminante (PLS-DA) es una variante que se utiliza cuando la Y es binaria.


\subsubsection{Reconocimiento}

Una  vez  se  tienen  las  características  más  importantes de la imagen a analizar hay que determinar el  carácter correspondiente en este trabajo exploraremos la técnica de KNN.


\subsubsection{ K-NN (K vecinos más próximos)}

Es un método no paramétrico y supervisado, que dado un conjunto de objetos prototipo de los que ya se conoce su clase (es decir, dado un conjunto de caracteres de muestra) y dado un  nuevo objeto cuya clase no conocemos (imagen de un carácter a reconocer) se busca entre el conjunto de prototipos los “k” más parecidos a nuevo objeto. A este se le asigna la clase más numerosa entre los “k” objetos prototipo seleccionados.


\subsubsubsection{Entrenamiento y Test}
Conociendo el funcionamiento básico del método de clasificación de los “k vecinos más próximos” es obvio que para poder empezar a trabajar con este método es necesario reunir un conjunto de datos etiquetados, es decir, un conjunto de muestras prototipo con las clases a las que pertenecen. Esta recolección implica disponer de una base de datos de imágenes de los tipos de caracteres que posteriormente se esperen reconocer. A este conjunto de datos se le denomina conjunto de entrenamiento. Sin embargo, la fase de entrenamiento no solo consiste en la recopilación de estos datos, sino que, típicamente, los datos originales que se dedican al
entrenamiento deben ser preprocesados adecuadamente para obtener representaciones compactas y coherentes. Esto quiere decir que  las  imágenes deben ser segmentadas, normalizadas y transformadas para obtener los vectores de baja dimensionalidad que finalmente se almacenan como conjunto de entrenamiento.
Con este conjunto de entrenamiento ya construido, el clasificador “knn” ya puede ser utilizado para reconocer la clase de una nueva muestra. Esta es la fase de test y lógicamente, también aquí es necesario aplicar todo el preproceso descrito anteriormente a cada una de las nuevas muestras. Por lo tanto, aquí se ve la necesidad de disponer de métodos rápidos de realizar estas tareas de preproceso, puesto que la velocidad de reconocimiento dependerá, en parte, de ellos. En la práctica se tiene que este preproceso es posible realizarlo muy rápidamente, aunque justo a continuación aparece la parte del proceso de reconocimiento que normalmente más carga computacional conlleva, la clasificación.



Para cumplir con el objetivo del trabajo práctico, lo primero que desarrollamos fue la implementación del algoritmo de $K-NN$. Como mencionamos en la introducción esta tecnica permite dado un dado del que no conocemos a que clase pertenece, buscar entre el set de datos de imagenes etiquetadas las $k$ más parecidas, denomidadas $vecinas$. Luego una vez identificados determinar cual es la moda.

\subsection {Elección del K}

La mejor elección de $k$ depende fundamentalmente de los datos; generalmente, valores grandes de k reducen el efecto de ruido en la clasificación, pero crean límites entre clases parecidas. Un buen k puede ser seleccionado mediante una optimización de uso.

La exactitud de este algoritmo puede ser severamente degradada por la presencia de ruido o características irrelevantes, o si las escalas de características no son consistentes con lo que uno considera importante.


\subsection {Algorimo}


\begin{algorithm}
    \begin{algorithmic}[1]\parskip=2mm
        \caption{vector KNN (matriz etiquedados, matriz sinEtiquedar, int cantidadVecinos)}
        \STATE{vector etiquetas = vector(cant_filas(sinEtiquetar))}\\
        \STATE{for 1 to cant_filas(sinEtiquetar) do }\\
        \STATE{\quad etiquetas_{i} = encontrarEtiquetas(etiquedados, sinEtiquetar_{i}, cantidadVecinos)}\\
        \STATE{end for}\\
        \STATE{\RETURN etiquetas}\\
    \end{algorithmic}
\end{algorithm}


Para encontrar las etiquetas implementamos el siguiente algoritmo


\begin{algorithm}
    \begin{algorithmic}[1]\parskip=2mm
        \caption{int encontrarEtiquetas(matriz etiquetados, vector incognito, int cantidadVecinos)}
        \STATE{for 1 to size(incognito) do}\\
        \STATE{\quad resParcial = restaVectores(etiquedatos_{i}, incognito) }\\
        \STATE{\quad colaPrioridad.push((norma(resParcial), etiqueta(etiquetados_{i}))}\\
        \STATE{end for}\\
        \STATE{vector numeros = vector(10)}\\
        \STATE{while cantidadVecinos > 0 & noEsVacia(resultados) do}\\
        \STATE{\quad int elemento = primero(resultados.etiqueta)}\\
        \STATE{\quad numeros_{elemento}++}\\
        \STATE{end while}\\
        \STATE{\RETURN maximo(numeros)}
    \end{algorithmic}
\end{algorithm}


\subsection {Optimización con PCA}


El $Análisis de Componentes Principales$ utiliza una transformación lineal ortogonal de los datos para convertir un conjunto de variables, posiblemente correlacionadas, a un nuevo sistema de coordenadas conocidas estas como componentes principales tal que la mayor varianza de cualquier proyecci ́on de los datos queda ubicada como la primer coordenada, la segunda mayor varianza en la segunda posición, y asi sucesivamente.

En este sentido, PCA calcula la base más significativa para expresar nuestros datos.
De esta manera entonces, será fácil quedarnos con los λ componentes principales que concentran la mayor varianza y quitar el resto.

En la sección de experimentación, uno de los objetivos principales será buscar cual es el λ que concentra la mayor varianza de manera tal de optimizar el número de predicciones. A fines prácticos, lo que haremos es, a partir de nuestra base de datos de elementos etiquetados, será construir la matriz de covarianza M de tal manera que en la coordenada M_{i,j} se obtenga el valor de la covarianza del pixel i contra el pixel j.

Luego, utilizando el método de la potencia, procederemos a calcular los primeros λ autovectores de esta matriz. Una vez obtenidos los
autovectores, multiplicando cada elemento por los λ autovectores, obtendremos un nuevo set de datos.

Sobre este set de datos, ahora aplicaremos el algoritmo $KNN$ nuevamente y lo que esperamos ver es un mayor número de aciertos, ya que hemos quitado ruido del set de datos. Esto se suma a mejores tiempos de ejecuci ́on, ya que hemos reducido la dimensionalidad del problema.

Generalizando entonces, los supuestos de PCA son:

\begin{itemize}
    \item Linealidad: La nueva base es una combinación lineal de la base original
    \item Media y Varianza son estadísticos importantes: asume que estos estadísticos describen la distribución de los datos sobre el eje.
    \item Varianza alta tiene una dinámica importante: Varianza alta significa senal. Baja varianza significa ruido.
    \item Las componentes son ortonormales
\end{itemize}

Si algunas de estas características no es apropiada, PCA podría producir resultados pobres. Un hecho importante que debemos recordar: PCA devuelve una nueva base que es una combinación lineal de la base original, limitando el número de posibles bases que puedan ser encontradas.


\subsubsection {Cross-Validation}

Para medir la precisión de los resultados utilizamos la metodología de cross-validation. Esta consiste en tomar nuestra base de datos de entrenamiento y dividirla en k bloques. En una primera iteración se toma un bloque para testear y los bloques restantes para entrenar a nuestro modelo,observando los resultados obtenidos. En la siguiente, se toma el segundo bloque para testear y los
restantes como dataset de entrenamiento.
La metodología se repite k veces hasta iterar todo el conjunto de datos. Finalmente, se realiza la media aritmética de los resultados de cada iteración para obtener un único resultado de error y poder evaluar la performance del método de entrenamiento.

Esta técnica, que es una mejora de la técnica de holdout donde simplemente se divide el set de datos en dos conjuntos (uno para entrenamiento y otro para testing), trata de garantizar que los resultados obtenidos sean independientes de la partición de datos contra la que se est ́a evaluando porque ofrece el beneficio de que los parámetros del método de predicción no pueden ser ajustados exhaustivamentea casos particulares. Es por esto que se utiliza principalmente en situaciones de predicción, dado que intenta evitar que el aprendizaje se realice sobre un cuerpo de datos específico y busca obtener respuestas más generales.

La única desventaja que presenta es la necesidad esperable de correr los algoritmos en varias iteraciones, situación que puede tener un peso significativo si el método de predicción tiene un costo computacional muy alto durante el entrenamiento.


\subsubsection {Algoritmos para Optimización con PCA}

\begin{algorithm}
    \begin{algorithmic}[1]\parskip=2mm
        \caption{void PCA(matriz etiquetados, matriz sinetiquetar,int cantidadAutovectores)}
        \STATE{matriz covarianza = obtenerCovarianza(etiquetados)}\\
        \STATE{vector(vector) autovectores}\\
        \STATE{for 1 to cantidadAutovectores do}\\
        \STATE{\quad vector autovector=metodoDeLasPotencias(covarianza)}\\
        \STATE{\quad agregar(autovectores,autovector)}\\
        \STATE{\quad double lamda = encontrarAutovalor(autovector,covarianza)}\\
        \STATE{\quad multiplicarXEscalar(auovector,lamda)}\\
        \STATE{\quad restaMatrizVector(covarianza,auovector,lamda)}\\
        \STATE{end for}\\
        \STATE{\RETURN maximo(numeros)}
    \end{algorithmic}
\end{algorithm}


\begin{algorithm}
    \begin{algorithmic}[1]\parskip=2mm
        \caption{matriz obtenerCovarianza(matriz entrada,vector medias)}
        \STATE{matriz covarianza, vector nuevo}\\
        \STATE{for i=1 to size(medias) do}\\
        \STATE{for j=1 to cant filas(entrada) do}\\
        \STATE{\quad nuevoVector_{j} = entrada_{(j,i)} − medias_{i}}\\
        \STATE{end for}\\
        \STATE{agregar(covarianza,nuevoVector)}\\
        \STATE{end for}\\
        \STATE{for i=1 to cant filas(entrada) do}\\
        \STATE{for k=1 to cant filas(entrada) do}\\
        \STATE{covarianza_{i} = multiplicarVectorEscalar(covarianza_{k} , cantidadFilas(entrada))}\\
        \STATE{end for}\\
        \STATE{end for}\\
        \STATE{\RETURN covarianza}
    \end{algorithmic}
\end{algorithm}

Además implementamos los métodos auxiliares

\begin{algorithm}
    \begin{algorithmic}[1]\parskip=2mm
        \caption{Vector metodoDeLasPotencias(matriz covarianza,cantIteraciones)}
        \STATE{vector vectorInicial= vector(cant filas(covarianza))}\\
        \STATE{for 1 to cantIteraciones do}\\
        \STATE{\quad vector nuevo = multiplicar(covarianza,vectorInicial)}\\
        \STATE{\quad multiplicarEscalar(nuevo,1/norma(nuevo))}\\
        \STATE{\quad vectorInicial = nuevo}\\
        \STATE{end for}\\
        \STATE{\RETURN vectorInicial}
    \end{algorithmic}
\end{algorithm}



\begin{algorithm}
    \begin{algorithmic}[1]\parskip=2mm
        \caption{Vector medias(matriz entrada)}
        \STATE{for i=1 to cantColumnas(entrada) do}\\
        \STATE{\quad suma = 0 }\\
        \STATE{\quad for j=1 to cant columnas(entrada) do}\\
        \STATE{\quad \quad suma += entrada_{i,j}}\\
        \STATE{end for}\\
        \STATE{medias_{i} = suma/cantFilas(entrada)}\\
        \STATE{end for}\\
        \STATE{\RETURN medias}
    \end{algorithmic}
\end{algorithm}


\subsection {Optimización con PLS-DA}

\subsubsection {Algoritmos para Optimización con PLS-DA}
