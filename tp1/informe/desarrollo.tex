\subsection{Porcentaje de Victorias}
Comenzamos con este método y a lo largo del informe por cuestiones practicas lo denominaremos \textbf{WP}.

Tal como habíamos comentado anteriormente, la implementación consitirá en calcular el ranking $r_i$ de un equipo $i$ en base a sus partidos ganados ($w_i$) sobre los partidos totales. Debido a que los empates no son tenidos en cuenta en dicho método consideraremos los partidos totales como la suma de los ganados más los perdidos ($l_i$).

\begin{center}
    r_i = \frac{w_i}{w_i + l_i}. 
\end{center}

En este caso, el puntaje de un equipo no es afectado por los resultados de los demás participantes pero si influye directamente en que posición del ranking finalizara. 
Adicionalmente, podemos afirmar que cada partido perdido incrementa la variable $l_i$ (partidos totales), la cual actua como divisor, generando en consecuencia un score inferior; siendo así también un modificador de la posición sobre el ranking.

A continuación mostraremos esto con un ejemplo:

Supongamos que arrancamos un torneo con 3 equipos \textbf{A}, \textbf{B} y \textbf{C}, como ninguno disputo ningún partido tendrán 0 en la tabla de posiciones.

Supongamos los siguientes partidos y observemos como quedan posicionados:

\begin{itemize}
\item \textbf{A} vs \textbf{B} $=>$ GANA \textbf{A} 
\item \textbf{A} vs \textbf{C} $=>$ GANA \textbf{C} 
\end{itemize}
\\
\begin{center}
    \begin{tabular}{| 1 | 1 | 1 |}
    \hline
    Posición & Equipo & Ranking \\ \hline
    1 & C & 1 \\ \hline
    2 & A & 1/2  \\ \hline
    3 & B & 0 \\ 
    \hline
    \end{tabular}
\end{center}

Como puede observarse en la tabla, el equipo \textbf{B} quedo último ya que no gano ningún partido. La diferencia entre el equipo \textbf{A} y el equipo \textbf{C} es la cantidad de partidos jugados, ya que ambos ganaron la misma cantidad pero como mencionamos anteriormente como el divisor es más grande hace que el equipo \textbf{C} quede mejor posicionado.

Veamos que sucede si juegan el equipo \textbf{B} vs equipo \textbf{C} y gana el equipo \textbf{B}.

\begin{center}
    \begin{tabular}{| 1 | 1 | 1 |}
    \hline
    Posición & Equipo & Ranking \\ \hline
    1 & C & 1/2 \\ \hline
    2 & A & 1/2  \\ \hline
    3 & B & 1/2 \\
    \hline
    \end{tabular}
\end{center}

Como podemos ver, todos quedan con igual ranking y esto es ya que todos jugaron la misma cantidad de partidos y ganaron la misma cantidad de partidos.

Con este ejemplo se puede observar lo que mencionamos anteriormente, que si bien los partidos de los demás no afectan a cambiar el valor de un equipo dentro del ranking si puede afectar su posición, a su vez esta técnica no aporta mucha información respecto a la posibilidad de victoria en el siguiente encuentro y tampoco considera el ranking del rival enfrentado, con lo cual la posición del equipo en el ranking depende de los resultados de victorias del equipo. Por otro lado podemos ver que nunca mencionamos los empates con este método, por ende podríamos analizar como podríamos modelarlos y como afectaría el ranking.

\subsection{Sistema a resolver}
Veamos el otro método para obtener rankings para poder comparar y ver cual nos parece más justo a la hora de utilizarlos para competencias reales.
Comencemos armando el sistema de la Matriz de Colley basados en el paper de Colley\footnote{''\textbf{The Colley Matrix Explained}:Wesley N. Colley''}, este método está basado en la Regla de Laplace de sucesos y solo se requiere conocer un historial de partidos. 
El método de \textbf{CMM} propone construir una matriz C $\in$ $ R^{nxn}$ , un vector b $\in$ $ R^n$ tal que el ranking r $\in$ $R^n$ buscado sea la solución al sistema :
\hfill $Cr=b$ \hfill \stackon{}{\textbf{(1)}}
\\
Para cada equipo $i$ llamamos $n_i$ a la cantidad total de partidos jugados por el equipo $i$, $w_i$ como la cantidad victorias el equipo y, análogamente $l_i$ los partidos perdidos. Por último definimos $n_{i,j}$ la cantidad de partidos jugados entre el equipo $i$ y $j$. Dadas estas definiciones Colley propone armar la matriz como: 

\[ C_{i,j} =
    \begin{cases}
        -n_{i,j}       & \quad \text{si }  \text{$i$ $\neq$ $j$} \\
        2+n_i & \quad \text{si } \text{$i$ $=$ $j$ }\\
    \end{cases}\hfill \stackon{}{\textbf{(2)}}
$$
  $$  \]
                    
y para cada posición correspondiente al equipo $i$, el $b_i$ = 1 +  ($w_i$ - $l_i$) / 2.

Si vemos la definición de la matriz, podemos notar que es estrictamente diagonal dominante ya que la suma de todas las posiciones de la fila, excepto el elemento correspondiente a la diagonal, suman la cantidad de partidos totales $n_i$, siendo este es el valor de la posición en la diagonal menos 2. 
Por otro lado, tiene la particularidad de ser simétrica y definida positiva y, y como consecuencia de todas estas propiedades podemos decir que la matriz es no singular, permitiéndonos utilizar la eliminación gaussiana sin pivoteo y obtener la factorización de Cholesky de dicha matriz.

Nos centraremos en ambos métodos para la resolución del sistema \textbf{}\textbf{(1)} que planteamos arriba, combinándolo con algoritmos de sustitución como son el back substitution y forward substitution.

Volviendo a las características del método de CMM, a simple vista podemos ver que al igual que en $WP$ no se tiene en cuenta el margen de victoria, no modela los empates y tampoco nos da indicio de como impactara en el ranking cada uno de los resultados de los partidos.

En la sección de experimentos, veremos si con este método importa contra quien se gana y contra quien se pierde basándonos en las posiciones y analizaremos que pasara con los empates. 

A priori nuestra intuición nos dice que importa más a quien se le gana, pensamos que no es lo mismo ganarle al que esta último que ganarle al que esta primero y en esto nos vamos a basar para realizar nuestras experimentaciones en las secciones de más adelante, para esto utilizaremos un algoritmo greedy tomando al equipo que salga último en el ranking y haciéndolo jugar con otro equipo que este mejor posicionado que él. Utilizaremos 2 heurísticas distintas con el fin de obtener una mejor posición, la primera será contra el inmediato siguiente en el ranking y la otra contra el que este primero, siempre tomando el ranking que resulta luego de cada partido.

\subsubsection{Eliminación Gaussiana}
Sea $C$ es la Matriz de Colley y $c_i$ los elementos de la misma , buscamos obtener el vector $r$ mostrado en la ecuación del sistema a resolver. 

\begin{center}
$Cr=b$
\end{center}

El algoritmo de Eliminación Gaussiana es útil para la resolución de sistemas lineales, con el cual mediante intercambio y combinaciones lineales entre filas podemos llegar a una matriz escalonada por reglones con ceros debajo de los elementos de la diagonal.\\
Una vez triangulada la matriz, mediante el algoritmo denominado back substitution se realiza el cálculo para obtener el valor de las incógnitas tomando cada elemento como\\

\begin{center}
$x_i = (b_i - \sum\limits_{j = i + 1}^n c_{ij}x_i) / c_{ii}$ \\
\end{center}

La matriz que nos propone armar el sistema de Colley es no singular y diagonal dominante, como mencionamos anteriormente, por lo que sabemos que en la diagonal nunca nos vamos a encontrar con un 0, por lo que en el algoritmo nos vamos a saltear esa validación ya que el intercambio de filas no ocurrirá nunca.

A continuación presentamos un pseudo código del algoritmo de eliminación gaussiana que utilizamos en nuestra implementación, para el mismo tomamos como base el seudocódigo del libro de Burden \footnote{''\textbf{Análisis Numérico }:Richard L. Burden & J. Douglas Faires''},

\begin{algorithm}
    \begin{algorithmic}[1]\parskip=2mm  
        \caption{vector Gauss(matriz A, vector b)}
        \STATE{Para $k$=$1...n-1$}\\
        \STATE{\quad Para $i$=$1...n-1$}\\
        \STATE{\quad\quad Se toma el elemento $a_{k,k}$ como pivot}\\
        \STATE{\quad\quad Para $j = i+1,...n$}\\
        \STATE{\quad \quad \quad $a_{i,j}  = a_{i,j} - a_{i,j} * (a_{i,k} / a_{k,k})$}\\
        \STATE{\quad \quad \quad $b_{i}  = b_{i} - a_{i,j} * (a_{i,k} / a_{k,k})$}\\
        \STATE{return backwardSubstitution(A,b)}
    \end{algorithmic}
\end{algorithm}


\begin{algorithm}
    \begin{algorithmic}[1]\parskip=2mm
        \caption{vector backSubstitution(matriz A, vector b)}
        \STATE{$x = vector(Cantidad\_Columnas(A))$}\\
        \STATE{$x_{n} = a_{n,n+1}/a_{n,n}$}\\
        \STATE{para $i=n-1..1$}\\
        \STATE{\quad para $j=i+1..n$}\\
        \STATE{\quad\quad $ sum +=a_{i,j}*x_{j}$}\\
        \STATE{\quad $x_i = \frac{(b_i -sum}{a_{i,i}}$}
        \STATE{\RETURN $x$}\\
    \end{algorithmic}
\end{algorithm}

Este algoritmo tiene complejidad temporal de $O(n^{3})$ para la eliminación Gaussiana y $O(n^{2})$ para el algoritmo de back substitution dando un total de $O(n^{3})$ como consto temporal para obtener una solución a nuestro sistema de ecuaciones lineales.

\newpage
\subsubsection{Cholesky}

La Matriz del sistema a resolver cuenta, como mencionamos anteriormente, la propiedad de ser simétrica y definida positiva. Estas condiciones nos garantizan que la misma tenga otra factorización del tipo $C=LL^{t}$, esta se denomina la factorización de Cholesky, compuesta por una matriz Triangular inferior $L$ y la transpuesta de la misma matriz $L^{t}$. Donde las posiciones de esta matriz están compuestas de esta manera

$l_{ji} =
\left\{
    \begin{array}{lcc}
        \sqrt{c{_i}{_i} - \sum\limits_{k=1}^{i-1} (L_{ik})^2} & si & i = j \\
        \\ \frac{c_{ji} - \sum\limits_{k=1}^{i-1} L_{jk}l_{ik}}{l_{ii}} & si & i \neq j \\
    \end{array}
\right.$ \\

Una vez obtenido el sistema $LL^tx=b$ se procede a hacer back substituion para resolver $L^tx=y$, luego mediante forward substitution se obtiene la solución de $Ly=b$.

La implementación de la factorización de Cholesky que elegimos, al igual que la eliminación gaussiana, se encuentra en el libro \textbf{Burden}.
Este seudocódigo representa nuestra implementación sobre la factorización de Cholesky.

\begin{algorithm}
    \begin{algorithmic}[1]\parskip=2mm
        \caption{vector Cholesky(matriz A, vector b)}
        \STATE{$l_{1,1} = \sqrt{a_{1,1}}$}\\
        \STATE{Para j = 2,...n}
        \STATE{\quad $ l_{j,1} = a_{j,1} / l_{1,1}$}
        \STATE{Para $i = 2,...n-1$}\\
        \STATE{\quad $l_{i,i}  = (a_{i,i} - \sum_{k=1}^{i-1}{l^2_{i,k}}^{1/2})$}\\
        \STATE{\quad Para $j = i+1,...n1$}\\
        \STATE{\quad\quad $l_{j,i}  = (a_{j,i} - \sum_{k=1}^{i-1}{l_{j,k} l_{i,k}} / l_{i,i})$}\\
        \STATE{$l_{n,n}  = (a_{n,n} - \sum_{k=1}^{n-1}{l^2_{n,k}}$)}\\
        \STATE{y $=$ backSubstitution(L,b)}
        \STATE{x = forwardSubstitution(L,y)}
        \STATE{return x}
    \end{algorithmic}
\end{algorithm}

Como se detallo en el algoritmo, se utiliza el metodo de backSubstitution al igual que en la eliminacion gaussiana y por otro lado se utiliza el forward subsittution que es similar al metodo mencionado anteriormente para la obtencion de los valores de las incognitas. El los algoritmos implementados no estan como funciones independientes por una mala desicion de dise\~no, pero pudimos haber aprovechado la ventaja para no repetir codigo.

El algoritmo de este metodo es el siguiente:
\begin{algorithm}
    \begin{algorithmic}[1]\parskip=2mm
        \caption{vector forwardSubstitution(matriz L, vector b)}
        \STATE{$y_1=\frac{b_1}{l_{1,1}}$}\\ 
        \STATE{Para i$=$2..n}\\
        \STATE{\quad Para j$=$1..i-1}\\
        \STATE{\quad\quad sum=$l_{i,j}*y_j$}\\
        \STATE{\quad\quad $y_i=\frac{b_i-sum}{l_{i,i}}$}\\
        \STATE{return y}
      \end{algorithmic}
\end{algorithm}

La factorizacion de Cholesky tiene complejidad temporal de $O(n^3)$ para Cholesky y $O(n^2)$ para los métodos de forward substitution y back substitution, quedando un total de $O(n^3)$.

\newpage
